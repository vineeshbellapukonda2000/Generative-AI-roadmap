ğŸ“œ Project Journey & Learnings
ğŸ Phase 1 â€” Foundation & Dataset Understanding

Started with the movie dataset (movies.csv), exploring columns such as Title, Genre, Description, Year, Rating, Runtime, Revenue, and Metascore.

Preprocessed and cleaned the dataset using Pandas, preparing it for both structured queries and semantic embedding generation.

Built the first version of the ingest.py script to chunk movie descriptions and create embeddings using OpenAIâ€™s embedding model.

Stored embeddings in a FAISS vector index, setting the foundation for retrieval.

ğŸ§± Phase 2 â€” Building the RAG Pipeline

Developed the retrieval system (retriever.py) that loads FAISS, performs vector similarity search, and returns relevant chunks.

Implemented semantic search that allowed the model to answer descriptive questions such as:

â€œWhat is the movie Inception about?â€ or â€œWho acted in The Dark Knight?â€

Initially, the system could only answer questions directly related to text chunks from the dataset, which we verified successfully through multiple test queries.

âš™ï¸ Phase 3 â€” Introducing Structured Query Handling

Identified a major limitation: the model couldnâ€™t handle numeric or factual questions (like rating, runtime, revenue, or release year).

Designed a new module structured.py to query structured data directly from the CSV using Pandas.

Implemented regex-based intent recognition to identify patterns like:

â€œWhat is the rating of ___?â€

â€œHow long is ___?â€

â€œWhen was ___ released?â€

This made the system capable of handling fact-based queries with precision.

ğŸ§  Phase 4 â€” Merging Structured + RAG Pipelines

Combined both systems into a Hybrid Intelligent Layer (main.py) that routes questions automatically:

If the query involves numeric/factual data â†’ handled by Structured Lookup.

If itâ€™s descriptive/contextual â†’ handled by RAG + LLM.

Implemented fusion logic to handle multi-intent questions, e.g.

â€œWhat is the rating and genre of The Dark Knight Rises?â€
This required merging structured data (rating) with retrieved chunks (genre), which we achieved using router.py.

ğŸ§© Phase 5 â€” Fine-Tuning the Interaction Layer

Created robust context-building logic in _to_context() for better LLM responses.

Improved query parsing (guess_title_from_query, parse_intents) to handle flexible question structures and capitalization.

Added OpenAI GPT-4o-mini as the reasoning model for final answer generation.

Enforced strict instruction in the LLM prompt:

â€œAnswer using only the context. If the answer isnâ€™t in context, say I donâ€™t know.â€

This made the responses accurate, interpretable, and context-bound.

ğŸ§— Struggles & How We Overcame Them

Challenge	Description	How We Solved It
FAISS and file path errors	Early on, file directories like level_03/mini_rag caused â€œno such fileâ€ errors due to spaces in folder names.	Fixed by using quoted paths and relative file references (Path(__file__).parent).
Mixing Structured + RAG answers	Initially, LLM returned only one type of answer (either numeric or descriptive).	Created a fusion router that merges structured lookups and semantic retrieval results before prompting the LLM.
Mislabeling (Rating vs Metascore)	The LLM often confused Rating (0â€“10) and Metascore (0â€“100).	Explicitly added prompt constraints in main.py to distinguish between the two scales.
â€œI donâ€™t knowâ€ responses	LLM returned â€œI donâ€™t knowâ€ for queries slightly mismatched to dataset text.	Enhanced semantic search recall (MMR) and added fallback descriptive prompts.
Permission/Conda path issues	Encountered Mac â€œpermission deniedâ€ and environment confusion while navigating paths.	Fixed by using chmod -R 755 and quoting all conda/env paths.
Complex multi-intent questions	LLM ignored second intent (e.g., genre + rating).	Implemented intent parsing and joined both structured and RAG responses.
ğŸ§¾ Key Takeaways

Learned how to combine structured data retrieval and unstructured RAG search into a unified conversational system.

Understood how embeddings, FAISS, and semantic similarity enable question answering beyond keywords.

Built an architecture capable of fusing symbolic (structured) and semantic (vector) intelligence â€” a real-world foundation for enterprise knowledge assistants.

Practiced debugging, environment management, and prompt engineering for reliable hybrid AI systems.