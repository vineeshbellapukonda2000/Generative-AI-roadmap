GenAI Cheat Sheet â€“ Level 2: RAG, Embeddings, Vector Stores, Memory


ðŸ”¹ Embeddings
Converts text â†’ high-dimensional vector

Captures semantic meaning

Models: sentence_transformers, text-embedding-3-small, all-MiniLM-L6-v2

Code:

python
Copy
Edit
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
vector = model.encode("What are the best exercises for biceps?")

ðŸ”¹ Cosine Similarity
Measures how close two vectors are (angle-based)

Used to retrieve relevant documents


ðŸ”¹ Vector Stores
Stores embedded documents

Supports similarity search

Popular Stores:

FAISS (local, fast)

ChromaDB (simple, Pythonic)

Pinecone, Weaviate (cloud-based)


ðŸ”¹ RAG (Retrieval-Augmented Generation)
Pipeline:

markdown
Copy
Edit
1. User Query â†’ 
2. Embed Query â†’ 
3. Vector Store Search (cosine similarity) â†’ 
4. Retrieve top-k documents â†’ 
5. Combine with prompt â†’ 
6. Send to LLM (GPT) â†’ 
7. Generate Answer
Goal: Use retrieved knowledge to reduce hallucination and improve relevance.


ðŸ”¹ Memory in GenAI
Type	Purpose	Tool
ConversationBufferMemory	Short-term memory (last N turns)	LangChain
VectorStoreRetrieverMemory	Long-term memory (semantic search)	FAISS + LangChain
CombinedMemory	Uses both	LangChain


ðŸ”¹ LangChain Components
Component	Description
Embeddings	Converts text into vectors
VectorStore	Stores/retrieves documents
Retriever	Interface to search vector DB
Memory	Tracks chat context
LLMChain	Runs query + context through LLM

ðŸ”¹ Chunking
Break large docs into small chunks before embedding

Use overlap (e.g., 500 tokens with 50 overlap) to preserve meaning


ðŸ”¹ Best Practices
Use same embedding model for both docs and queries

Fine-tune chunk size and retrieval k

Avoid prompt flooding (keep context under LLM token limits)

ðŸ”¹ Example Use Cases
Chatbots grounded in personal notes

Legal assistants with case law documents

Fitness/Nutrition coaches with custom plans

Movie recommenders from reviews and tags

